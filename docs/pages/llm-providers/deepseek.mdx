# DeepSeek-V3 Setup

Use DeepSeek-V3 with agentful for 8-10x cost savings.

## Quick Start

```bash
# Install LiteLLM proxy
pip install 'litellm[proxy]'

# Get API key from https://platform.deepseek.com
export DEEPSEEK_API_KEY=sk-...

# Start proxy
litellm --model deepseek/deepseek-chat --drop_params --port 4000

# Configure Claude Code
export ANTHROPIC_BASE_URL=http://localhost:4000
export ANTHROPIC_API_KEY=$DEEPSEEK_API_KEY

claude
/agentful-start
```

## Get API Key

1. Visit [platform.deepseek.com](https://platform.deepseek.com)
2. Sign up → API Keys → Create
3. Copy key (starts with `sk-...`)

## Configure LiteLLM

**Docker:**

```bash
docker run -d \
  --name litellm-deepseek \
  -p 4000:4000 \
  -e DEEPSEEK_API_KEY=$DEEPSEEK_API_KEY \
  ghcr.io/berriai/litellm:main-latest \
  --model deepseek/deepseek-chat \
  --drop_params
```

**Or create `litellm_config.yaml`:**

```yaml
model_list:
  - model_name: deepseek-chat
    litellm_params:
      model: deepseek/deepseek-chat
      api_key: os.environ/DEEPSEEK_API_KEY
      drop_params: true

litellm_settings:
  drop_params: true
  cache: true
```

```bash
litellm --config litellm_config.yaml --port 4000
```

## Configure Claude Code

**Persistent** (`~/.claude/settings.json`):

```json
{
  "environmentVariables": {
    "ANTHROPIC_BASE_URL": "http://localhost:4000",
    "ANTHROPIC_API_KEY": "your_deepseek_api_key",
    "ANTHROPIC_DEFAULT_SONNET_MODEL": "deepseek-chat"
  }
}
```

**Session-based:**

```bash
export ANTHROPIC_BASE_URL=http://localhost:4000
export ANTHROPIC_API_KEY=$DEEPSEEK_API_KEY
claude
```

## Model Variants

- **deepseek-chat** - Standard model, use for 90% of tasks
- **deepseek-reasoner** - Extended reasoning (2x cost, still 84% cheaper than Claude)

## Switch Back to Claude

```bash
unset ANTHROPIC_BASE_URL
export ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
claude
```

## Resources

- [DeepSeek Platform](https://platform.deepseek.com)
- [API Docs](https://api-docs.deepseek.com)
- [LiteLLM Docs](https://docs.litellm.ai)
